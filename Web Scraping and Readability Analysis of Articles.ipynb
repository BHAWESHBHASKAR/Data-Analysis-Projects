{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVxVcA9wREfD9MeikjfeLN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BHAWESHBHASKAR/Data-Analysis-Projects/blob/main/Web%20Scraping%20and%20Readability%20Analysis%20of%20Articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGlSZ1hUKhDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7a35d9-e33f-4273-d4fe-15eeed9a1347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqw6ZB7oRxaD",
        "outputId": "4d491f90-31ad-498b-9132-c9333cbeca3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.15.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textstat import syllable_count"
      ],
      "metadata": {
        "id": "TzaEL1dnTb72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = 'Input.xlsx'\n",
        "df_input = pd.read_excel(input_file)\n"
      ],
      "metadata": {
        "id": "B319fmJRTeg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_text(url):\n",
        "    response = requests.get(url)\n",
        "    html_content = response.text\n",
        "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "    article = soup.find(\"article\")\n",
        "    if article:\n",
        "        title = article.find(\"h1\").get_text().strip()\n",
        "        text = article.get_text(separator='\\n').strip()\n",
        "        return title, text\n",
        "    else:\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "PxKylK66TgtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('StopWords_Generic.txt', 'r') as file:\n",
        "    stop_words_generic = set(file.read().splitlines())\n",
        "with open('StopWords_Auditor.txt', 'r') as file:\n",
        "    stop_words_auditor = set(file.read().splitlines())\n"
      ],
      "metadata": {
        "id": "HDhPGU-MTi6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_data = []\n"
      ],
      "metadata": {
        "id": "mheRtUitUXld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df_input.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    title, text = extract_article_text(url)\n",
        "\n",
        "    if text:\n",
        "        tokens = word_tokenize(text)\n",
        "        cleaned_tokens = [word.lower() for word in tokens if word.lower() not in stop_words_generic and word.lower() not in stop_words_auditor]\n",
        "\n",
        "        num_words = len(cleaned_tokens)\n",
        "        num_sentences = text.count('.') + text.count('!') + text.count('?')\n",
        "        average_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "        complex_words = [word for word in cleaned_tokens if len(word) > 2]\n",
        "        percentage_complex_words = len(complex_words) / num_words if num_words > 0 else 0\n",
        "        fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
        "\n",
        "        average_words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n",
        "        complex_word_count = len(complex_words)\n",
        "        syllable_per_word = sum(syllable_count(word) for word in cleaned_tokens) / num_words if num_words > 0 else 0\n",
        "\n",
        "        personal_pronouns = sum(1 for word in cleaned_tokens if word in {'i', 'we', 'my', 'ours', 'us'})\n",
        "\n",
        "        average_word_length = sum(len(word) for word in cleaned_tokens) / num_words if num_words > 0 else 0\n",
        "\n",
        "        output_data.append([\n",
        "            url_id,\n",
        "            row['URL'],\n",
        "            num_words,\n",
        "            num_sentences,\n",
        "            average_sentence_length,\n",
        "            percentage_complex_words,\n",
        "            fog_index,\n",
        "            average_words_per_sentence,\n",
        "            complex_word_count,\n",
        "            num_words,\n",
        "            syllable_per_word,\n",
        "            personal_pronouns,\n",
        "            average_word_length\n",
        "        ])\n",
        "    else:\n",
        "        output_data.append([url_id] + [row['URL']] + [float('nan')] * 11)\n"
      ],
      "metadata": {
        "id": "yJI1rYgbUnbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_output = pd.DataFrame(output_data, columns=[\n",
        "    'URL_ID',\n",
        "    'URL',\n",
        "    'WORD COUNT',\n",
        "    'SENTENCE COUNT',\n",
        "    'AVG SENTENCE LENGTH',\n",
        "    'PERCENTAGE OF COMPLEX WORDS',\n",
        "    'FOG INDEX',\n",
        "    'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "    'COMPLEX WORD COUNT',\n",
        "    'WORD COUNT',\n",
        "    'SYLLABLE PER WORD',\n",
        "    'PERSONAL PRONOUNS',\n",
        "    'AVG WORD LENGTH'\n",
        "])\n",
        "\n",
        "output_filename = 'Output.xlsx'\n",
        "df_output.to_excel(output_filename, index=False)\n"
      ],
      "metadata": {
        "id": "9_BzS5b8VBZn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}